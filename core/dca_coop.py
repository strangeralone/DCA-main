"""DCA + CoOp è®­ç»ƒå™¨æ¨¡å—

ç»§æ‰¿ DCA è®­ç»ƒå™¨ï¼Œä½¿ç”¨ CoOp (Context Optimization) è¿›è¡Œæç¤ºå­¦ä¹ ã€‚
ä¸ CLIP å›ºå®šæç¤ºä¸åŒï¼ŒCoOp é€šè¿‡å­¦ä¹ å¯ä¼˜åŒ–çš„æç¤ºå‘é‡æ¥é€‚åº”ç›®æ ‡åŸŸã€‚

æ ¸å¿ƒåŒºåˆ«ï¼š
- DCA_CLIP: å›ºå®šæç¤º "a photo of a {class}"
- DCA_CoOp: å¯å­¦ä¹ æç¤º [V1][V2]...[Vn]{class}
"""

import os
import os.path as osp
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from datetime import datetime
from tqdm import tqdm

import clip
from core.dca import DCATrainer, op_copy, lr_scheduler
from utils.loss import SKL, entropy, adentropy, class_balance, iid_loss
from utils.helpers import cal_acc, cal_acc_easy, obtain_label, obtain_label_easy, cal_acc_tta, cal_acc_easy_tta


class TextEncoder(nn.Module):
    """CLIP æ–‡æœ¬ç¼–ç å™¨å°è£…
    
    ç”¨äºå°†å¯å­¦ä¹ çš„æç¤ºå‘é‡ç¼–ç ä¸ºæ–‡æœ¬ç‰¹å¾ã€‚
    """
    def __init__(self, clip_model):
        super().__init__()
        self.transformer = clip_model.transformer
        self.positional_embedding = clip_model.positional_embedding
        self.ln_final = clip_model.ln_final
        self.text_projection = clip_model.text_projection
        self.dtype = clip_model.dtype

    def forward(self, prompts, tokenized_prompts):
        """å‰å‘ä¼ æ’­
        
        Args:
            prompts: ç»è¿‡ç»„åˆçš„æç¤ºåµŒå…¥ [n_cls, n_ctx + 1 + n_suffix, dim]
            tokenized_prompts: tokenized çš„æç¤ºï¼Œç”¨äºè·å– EOS ä½ç½®
        """
        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # [n_ctx + 1 + n_suffix, n_cls, dim]
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # [n_cls, n_ctx + 1 + n_suffix, dim]
        x = self.ln_final(x).type(self.dtype)

        # è·å– EOS token ä½ç½®çš„ç‰¹å¾
        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection

        return x


class PromptLearner(nn.Module):
    """å¯å­¦ä¹ çš„æç¤ºå‘é‡ï¼ˆå¯¹ç…§å®˜æ–¹ CoOp å®ç°ï¼‰
    
    CoOp çš„æ ¸å¿ƒï¼šå°†å›ºå®šæç¤º "a photo of a" æ›¿æ¢ä¸ºå¯å­¦ä¹ çš„å‘é‡ã€‚
    
    ç»“æ„ï¼š[SOS][V1][V2]...[Vn][CLASS][EOS]
    - SOS: èµ·å§‹ token
    - V1~Vn: å¯å­¦ä¹ çš„ä¸Šä¸‹æ–‡å‘é‡
    - CLASS: ç±»åˆ«åç§°çš„åµŒå…¥
    - EOS: ç»“æŸ token
    
    å‚è€ƒ: https://github.com/KaiyangZhou/CoOp/blob/main/trainers/coop.py
    """
    def __init__(self, clip_model, class_names, n_ctx=16, ctx_init="", class_token_position="end"):
        super().__init__()
        n_cls = len(class_names)
        dtype = clip_model.dtype
        ctx_dim = clip_model.ln_final.weight.shape[0]  # æç¤ºå‘é‡çš„ç»´åº¦
        device = next(clip_model.parameters()).device
        
        self.n_cls = n_cls
        self.n_ctx = n_ctx
        self.dtype = dtype
        self.class_token_position = class_token_position
        
        # åˆå§‹åŒ–å¯å­¦ä¹ çš„ä¸Šä¸‹æ–‡å‘é‡
        if ctx_init:
            # ä½¿ç”¨é¢„å®šä¹‰æ–‡æœ¬åˆå§‹åŒ–
            ctx_init = ctx_init.replace("_", " ")
            n_ctx = len(ctx_init.split(" "))
            prompt = clip.tokenize(ctx_init).to(device)
            with torch.no_grad():
                embedding = clip_model.token_embedding(prompt).type(dtype)
            ctx_vectors = embedding[0, 1:1+n_ctx, :]
            prompt_prefix = ctx_init
        else:
            # éšæœºåˆå§‹åŒ–ï¼ˆä¸å®˜æ–¹ä¸€è‡´ï¼šä½¿ç”¨æ ‡å‡†å·® 0.02ï¼‰
            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)
            nn.init.normal_(ctx_vectors, std=0.02)
            prompt_prefix = " ".join(["X"] * n_ctx)  # å ä½ç¬¦
        
        print(f'åˆå§‹åŒ–ä¸Šä¸‹æ–‡: "{prompt_prefix}"')
        print(f"ä¸Šä¸‹æ–‡ token æ•°é‡: {n_ctx}")
        
        # è®¾ç½®ä¸ºå¯å­¦ä¹ å‚æ•°
        self.ctx = nn.Parameter(ctx_vectors)
        
        # å‡†å¤‡ç±»åˆ«åç§°çš„åµŒå…¥ï¼ˆä¸å®˜æ–¹ä¸€è‡´ï¼‰
        classnames = [name.replace("_", " ") for name in class_names]
        # å®˜æ–¹ä½¿ç”¨ prompt_prefix + " " + name + "." æ ¼å¼
        prompts = [prompt_prefix + " " + name + "." for name in classnames]
        
        # Tokenize æ‰€æœ‰æç¤º
        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)
        with torch.no_grad():
            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)
        
        # ä¿å­˜ token åµŒå…¥çš„å„éƒ¨åˆ†
        self.register_buffer("token_prefix", embedding[:, :1, :])  # SOS
        self.register_buffer("token_suffix", embedding[:, 1 + n_ctx:, :])  # CLS, EOS
        
        self.n_ctx = n_ctx
        self.tokenized_prompts = tokenized_prompts
        # è®¡ç®—æ¯ä¸ªç±»åçš„ token é•¿åº¦ï¼ˆç”¨äº middle/front ä½ç½®ï¼‰
        self.name_lens = [len(clip.tokenize(name)[0].nonzero()) - 2 for name in classnames]  # å‡å» SOS å’Œ EOS
    
    def forward(self):
        """ç”Ÿæˆå®Œæ•´çš„æç¤ºåµŒå…¥ï¼ˆå¯¹ç…§å®˜æ–¹å®ç°ï¼‰
        
        Returns:
            prompts: [n_cls, context_length, dim]
        """
        ctx = self.ctx  # [n_ctx, dim]
        
        if ctx.dim() == 2:
            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)  # [n_cls, n_ctx, dim]
        
        prefix = self.token_prefix  # [n_cls, 1, dim]
        suffix = self.token_suffix  # [n_cls, *, dim]
        
        if self.class_token_position == "end":
            # æ ‡å‡† CoOp: [SOS][V1]...[Vn][CLASS][EOS]
            prompts = torch.cat([prefix, ctx, suffix], dim=1)
        
        elif self.class_token_position == "middle":
            # CLASS æ”¾åœ¨ä¸­é—´: [SOS][V1]...[Vn/2][CLASS][Vn/2+1]...[Vn][EOS]
            half_n_ctx = self.n_ctx // 2
            prompts = []
            for i in range(self.n_cls):
                name_len = self.name_lens[i]
                prefix_i = prefix[i:i+1, :, :]
                class_i = suffix[i:i+1, :name_len, :]
                suffix_i = suffix[i:i+1, name_len:, :]
                ctx_i_half1 = ctx[i:i+1, :half_n_ctx, :]
                ctx_i_half2 = ctx[i:i+1, half_n_ctx:, :]
                prompt = torch.cat([prefix_i, ctx_i_half1, class_i, ctx_i_half2, suffix_i], dim=1)
                prompts.append(prompt)
            prompts = torch.cat(prompts, dim=0)
        
        elif self.class_token_position == "front":
            # CLASS æ”¾åœ¨å‰é¢: [SOS][CLASS][V1]...[Vn][EOS]
            prompts = []
            for i in range(self.n_cls):
                name_len = self.name_lens[i]
                prefix_i = prefix[i:i+1, :, :]
                class_i = suffix[i:i+1, :name_len, :]
                suffix_i = suffix[i:i+1, name_len:, :]
                ctx_i = ctx[i:i+1, :, :]
                prompt = torch.cat([prefix_i, class_i, ctx_i, suffix_i], dim=1)
                prompts.append(prompt)
            prompts = torch.cat(prompts, dim=0)
        
        else:
            raise ValueError(f"Unknown class_token_position: {self.class_token_position}")
        
        return prompts


class DCACoOpTrainer(DCATrainer):
    """DCA + CoOp è®­ç»ƒå™¨ç±»
    
    ä¸ DCAClipTrainer çš„åŒºåˆ«ï¼š
    - CLIP æ–‡æœ¬ç‰¹å¾ä¸å†å›ºå®šï¼Œè€Œæ˜¯é€šè¿‡å¯å­¦ä¹ çš„æç¤ºå‘é‡ç”Ÿæˆ
    - éœ€è¦é¢å¤–ä¼˜åŒ–æç¤ºå‘é‡çš„å‚æ•°
    """
    
    def __init__(self, config):
        super().__init__(config)
        
        # CoOp é…ç½®
        coop_config = self.method_config.get('coop', {})
        self.clip_model_name = coop_config.get('model', 'ViT-B/16')
        self.n_ctx = coop_config.get('n_ctx', 16)
        self.ctx_init = coop_config.get('ctx_init', '')
        self.class_token_position = coop_config.get('class_token_position', 'end')
        
        # æŸå¤±æƒé‡
        self.coop_loss_weight = coop_config.get('loss_weight', 0.1)
        self.coop_temperature = coop_config.get('temperature', 100)
        self.coop_entropy_ratio = coop_config.get('entropy_ratio', 0.15)  # ç†µé˜ˆå€¼ = max_entropy * ratio
        self.coop_top_k_ratio = coop_config.get('top_k_ratio', 0.2)  # åˆ†æ­§ top k% ä¸ºéš¾æ ·æœ¬
        self.iid_weight = coop_config.get('iid_weight', 0.1)  # IID Loss æƒé‡
        
        # æ¨¡å‹ç»„ä»¶
        self.clip_model = None
        self.prompt_learner = None
        self.text_encoder = None
        
        # è°ƒè¯•é…ç½®
        self.debug = config.get('debug', False)
        self.verbose_loss = config.get('verbose_loss', True)
    
    def _load_clip_model(self):
        """åŠ è½½ CLIP æ¨¡å‹"""
        print(f"åŠ è½½ CLIP æ¨¡å‹: {self.clip_model_name}...")
        self.clip_model, _ = clip.load(self.clip_model_name, device=self.device, download_root='./vlm')
        
        # å†»ç»“ CLIP å›¾åƒç¼–ç å™¨
        for param in self.clip_model.parameters():
            param.requires_grad = False
        
        # åˆ›å»ºæ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = TextEncoder(self.clip_model)
        
        print("CLIP æ¨¡å‹å·²åŠ è½½!")
    
    def _init_prompt_learner(self, class_names):
        """åˆå§‹åŒ–å¯å­¦ä¹ çš„æç¤ºå‘é‡"""
        print(f"åˆå§‹åŒ– PromptLearner: n_ctx={self.n_ctx}, position={self.class_token_position}")
        self.prompt_learner = PromptLearner(
            self.clip_model, 
            class_names,
            n_ctx=self.n_ctx,
            ctx_init=self.ctx_init,
            class_token_position=self.class_token_position
        ).to(self.device)
        print(f"å¯å­¦ä¹ çš„æç¤ºå‘é‡å½¢çŠ¶: {self.prompt_learner.ctx.shape}")
    
    def _get_text_features(self):
        """é€šè¿‡å¯å­¦ä¹ çš„æç¤ºç”Ÿæˆæ–‡æœ¬ç‰¹å¾ï¼ˆæ¯æ¬¡è°ƒç”¨éƒ½ä¼šæ›´æ–°ï¼‰"""
        prompts = self.prompt_learner()
        tokenized_prompts = self.prompt_learner.tokenized_prompts
        text_features = self.text_encoder(prompts, tokenized_prompts)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        return text_features
    
    def _load_class_names(self):
        """åŠ è½½ç±»åˆ«åç§°"""
        data_dir = self.dataset_config.get('data_dir', './data/officehome')
        classname_file = self.dataset_config.get('classname_file', 'classname.txt')
        
        if classname_file:
            classname_path = osp.join(data_dir, classname_file)
            if osp.exists(classname_path):
                with open(classname_path, 'r') as f:
                    return [line.strip() for line in f.readlines()]
        
        return [f"class_{i}" for i in range(self.class_num)]
    
    def _write_config_to_log(self, log_file, task_type, source_idx, target_idx=None):
        """å°†é…ç½®ä¿¡æ¯å†™å…¥æ—¥å¿—æ–‡ä»¶"""
        super()._write_config_to_log(log_file, task_type, source_idx, target_idx)
        
        # è¿½åŠ  CoOp é…ç½®
        log_file.write("CoOp é…ç½®:\n")
        log_file.write(f"  coop.model: {self.clip_model_name}\n")
        log_file.write(f"  coop.n_ctx: {self.n_ctx}\n")
        log_file.write(f"  coop.ctx_init: {self.ctx_init or '(random)'}\n")
        log_file.write(f"  coop.class_token_position: {self.class_token_position}\n")
        log_file.write(f"  coop.loss_weight: {self.coop_loss_weight}\n")
        log_file.write(f"  coop.entropy_ratio: {self.coop_entropy_ratio}\n")
        log_file.write(f"  coop.top_k_ratio: {self.coop_top_k_ratio}\n")
        log_file.write(f"  debug: {self.debug}\n")
        log_file.write(f"  verbose_loss: {self.verbose_loss}\n")
        log_file.write("=" * 60 + "\n\n")
        log_file.flush()
    
    def _collect_high_confidence_samples(self, loader):
        """æ”¶é›†é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼ˆç”¨äºä¸¤é˜¶æ®µ Prompt Tuningï¼‰
        
        å‚è€ƒ DIFO: åªä½¿ç”¨é«˜ç½®ä¿¡åº¦æ ·æœ¬è®­ç»ƒ Promptï¼Œé¿å…å™ªå£°å¹²æ‰°ã€‚
        
        Returns:
            high_conf_indices: é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„ç´¢å¼•åˆ—è¡¨
            high_conf_probs: å¯¹åº”çš„ DCA è½¯æ ‡ç­¾
            high_conf_images: å¯¹åº”çš„å›¾åƒè·¯å¾„æˆ–æ•°æ®
        """
        self.netG.eval()
        self.netF.eval()
        self.netC.eval()
        self.netD.eval()
        
        all_probs = []
        all_indices = []
        
        with torch.no_grad():
            for inputs, _, idx in loader:
                inputs = inputs.to(self.device)
                
                # è·å–åŒåˆ†ç±»å™¨é¢„æµ‹
                features_d = self.netG(inputs)
                features = self.netF(features_d)
                outputs1 = self.netC(features)
                outputs2 = self.netD(features_d)
                
                softmax_out1 = F.softmax(outputs1, dim=1)
                softmax_out2 = F.softmax(outputs2, dim=1)
                
                # å¹³å‡é¢„æµ‹ä½œä¸ºè½¯æ ‡ç­¾
                avg_probs = (softmax_out1 + softmax_out2) / 2
                
                all_probs.append(avg_probs.cpu())
                all_indices.append(idx)
        
        all_probs = torch.cat(all_probs, dim=0)
        all_indices = torch.cat(all_indices, dim=0)
        
        # è®¡ç®—ç†µï¼Œé€‰æ‹©ä½ç†µï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰æ ·æœ¬
        entropy = -torch.sum(all_probs * torch.log(all_probs + 1e-8), dim=1)
        max_entropy = np.log(self.class_num)
        
        # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼ï¼Œé€‰æ‹©ç†µä½äºé˜ˆå€¼çš„æ ·æœ¬
        coop_config = self.method_config.get('coop', {})
        high_conf_threshold = coop_config.get('high_conf_threshold', 0.3)
        threshold = max_entropy * high_conf_threshold
        
        high_conf_mask = entropy < threshold
        high_conf_indices = all_indices[high_conf_mask]
        high_conf_probs = all_probs[high_conf_mask]
        
        print(f"  é«˜ç½®ä¿¡åº¦æ ·æœ¬: {len(high_conf_indices)}/{len(all_indices)} "
              f"({100*len(high_conf_indices)/len(all_indices):.1f}%)")
        
        return high_conf_indices, high_conf_probs
    
    def _tune_prompt_only(self, dset_loaders, high_conf_indices, high_conf_probs, log_file=None):
        """ç‹¬ç«‹ Prompt Tuning é˜¶æ®µï¼ˆé˜¶æ®µ 1ï¼‰
        
        åªè®­ç»ƒ Prompt Learnerï¼Œå†»ç»“æ‰€æœ‰å…¶ä»–ç½‘ç»œã€‚
        ä½¿ç”¨ IID Loss è®© CLIP è¾“å‡ºå¯¹é½é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„ DCA é¢„æµ‹ã€‚
        
        Args:
            dset_loaders: æ•°æ®åŠ è½½å™¨å­—å…¸
            high_conf_indices: é«˜ç½®ä¿¡åº¦æ ·æœ¬ç´¢å¼•
            high_conf_probs: é«˜ç½®ä¿¡åº¦æ ·æœ¬çš„ DCA è½¯æ ‡ç­¾
            log_file: æ—¥å¿—æ–‡ä»¶
        """
        coop_config = self.method_config.get('coop', {})
        prompt_tuning_steps = coop_config.get('prompt_tuning_steps', 10)
        prompt_lr = self.config.get('lr', 0.01) * 0.1
        
        print(f"  å¼€å§‹ç‹¬ç«‹ Prompt Tuning ({prompt_tuning_steps} steps)...")
        
        # åˆ›å»ºé«˜ç½®ä¿¡åº¦æ ·æœ¬çš„ç´¢å¼•åˆ°è½¯æ ‡ç­¾çš„æ˜ å°„
        idx_to_prob = {idx.item(): prob for idx, prob in zip(high_conf_indices, high_conf_probs)}
        
        # è®¾ç½® Prompt ä¼˜åŒ–å™¨
        optimizer_prompt = optim.SGD(
            [{"params": self.prompt_learner.ctx, "lr": prompt_lr}],
            momentum=0.9, weight_decay=5e-4
        )
        
        # å†»ç»“æ‰€æœ‰ç½‘ç»œå‚æ•°ï¼ˆåªè®­ç»ƒ Promptï¼‰
        self.netG.eval()
        self.netF.eval()
        self.netC.eval()
        self.netD.eval()
        
        # ç¡®ä¿ Prompt Learner å¯è®­ç»ƒ
        self.prompt_learner.train()
        for param in self.prompt_learner.parameters():
            param.requires_grad = True
        
        step = 0
        iter_loader = iter(dset_loaders["target"])
        total_iid_loss = 0.0
        
        while step < prompt_tuning_steps:
            try:
                inputs, _, tar_idx = next(iter_loader)
            except StopIteration:
                iter_loader = iter(dset_loaders["target"])
                inputs, _, tar_idx = next(iter_loader)
            
            # ç­›é€‰å‡ºé«˜ç½®ä¿¡åº¦æ ·æœ¬
            batch_mask = torch.tensor([i.item() in idx_to_prob for i in tar_idx])
            if not batch_mask.any():
                continue
            
            inputs_high_conf = inputs[batch_mask].to(self.device)
            selected_idx = tar_idx[batch_mask]
            
            # è·å–å¯¹åº”çš„ DCA è½¯æ ‡ç­¾
            dca_soft_labels = torch.stack([idx_to_prob[i.item()] for i in selected_idx]).to(self.device)
            
            # è·å– CLIP å›¾åƒç‰¹å¾ï¼ˆå†»ç»“ï¼‰
            clip_input = F.interpolate(inputs_high_conf, size=(224, 224), mode='bicubic')
            with torch.no_grad():
                image_features = self.clip_model.encode_image(clip_input)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            
            # è·å–å¯å­¦ä¹ çš„æ–‡æœ¬ç‰¹å¾ï¼ˆæœ‰æ¢¯åº¦ï¼‰
            text_features = self._get_text_features()
            
            # è®¡ç®— CLIP é¢„æµ‹
            clip_logits = image_features @ text_features.T
            clip_probs = F.softmax(clip_logits.float() * self.coop_temperature, dim=1)
            
            # IID Loss: è®© CLIP è¾“å‡ºä¸ DCA è½¯æ ‡ç­¾å¯¹é½
            loss_iid = iid_loss(clip_probs, dca_soft_labels)
            
            optimizer_prompt.zero_grad()
            loss_iid.backward()
            optimizer_prompt.step()
            
            total_iid_loss += loss_iid.item()
            step += 1
        
        avg_iid_loss = total_iid_loss / max(step, 1)
        print(f"  Prompt Tuning å®Œæˆ, å¹³å‡ IID Loss: {avg_iid_loss:.4f}")
        
        if log_file:
            log_file.write(f"  Prompt Tuning: {step} steps, avg IID Loss = {avg_iid_loss:.4f}\n")
            log_file.flush()
        
        # ç¼“å­˜è®­ç»ƒåçš„æ–‡æœ¬ç‰¹å¾
        with torch.no_grad():
            self.cached_text_features = self._get_text_features()
        
        return avg_iid_loss
    
    def train_target(self, source_idx, target_idx, log_file=None):
        """ç›®æ ‡åŸŸè‡ªé€‚åº”ï¼ˆå¸¦ CoOp æç¤ºå­¦ä¹ ï¼‰"""
        start_time = datetime.now()
        task_name = f"{self.domains[source_idx]} -> {self.domains[target_idx]}"
        print(f"ç›®æ ‡åŸŸé€‚åº” (å¸¦ CoOp æç¤ºå­¦ä¹ ): {task_name}...")
        
        # æ„å»ºç½‘ç»œ
        self._build_networks()
        
        # åŠ è½½ CLIP æ¨¡å‹
        self._load_clip_model()
        
        # åˆå§‹åŒ–å¯å­¦ä¹ çš„æç¤ºå‘é‡
        class_names = self._load_class_names()
        self._init_prompt_learner(class_names)
        
        # åŠ è½½æºåŸŸé¢„è®­ç»ƒæ¨¡å‹
        source_dir = self._get_output_dir(source_idx)
        self.netG.load_state_dict(torch.load(osp.join(source_dir, "source_G.pt"), map_location=self.device))
        self.netF.load_state_dict(torch.load(osp.join(source_dir, "source_F.pt"), map_location=self.device))
        self.netC.load_state_dict(torch.load(osp.join(source_dir, "source_C.pt"), map_location=self.device))
        self.netD.load_state_dict(torch.load(osp.join(source_dir, "source_D.pt"), map_location=self.device))
        
        # åŠ è½½ç›®æ ‡åŸŸæ•°æ®
        dset_loaders, num_samples = self._load_target_data(source_idx, target_idx)
        
        # ä»é…ç½®è·å–å‚æ•°
        target_config = self.method_config.get('target', {})
        max_epoch = target_config.get('max_epoch', 15)
        interval = target_config.get('interval', 10)
        
        # æ—©åœé…ç½®
        early_stop_patience = target_config.get('early_stop_patience', 3)
        early_stop_enabled = target_config.get('early_stop', True)
        
        lamda = self.method_config.get('lamda', 0.45)
        cls_par = self.method_config.get('cls_par', 0.15)
        alpha = self.method_config.get('alpha', 0.5)
        mix = self.method_config.get('mix', 0.5)
        lr = self.config.get('lr', 0.01)
        
        # è¾“å‡ºç›®å½•
        output_dir = self._get_output_dir(source_idx, target_idx)
        os.makedirs(output_dir, exist_ok=True)
        
        # æ—¥å¿—æ–‡ä»¶
        if log_file is None:
            log_file = open(osp.join(output_dir, "log.txt"), "w")
        
        # å†™å…¥é…ç½®ä¿¡æ¯
        self._write_config_to_log(log_file, 'target', source_idx, target_idx)
        
        # å†»ç»“ netC
        self.netC.eval()
        self.netD.train()
        for k, v in self.netC.named_parameters():
            v.requires_grad = False
        
        # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆä¸¤é˜¶æ®µç­–ç•¥ï¼šåªåŒ…å« DCA ç½‘ç»œï¼ŒPrompt åœ¨ç‹¬ç«‹é˜¶æ®µè®­ç»ƒï¼‰
        param_group_g = []
        param_group_d = []
        
        for k, v in self.netG.named_parameters():
            param_group_g += [{"params": v, "lr": lr * 0.1}]
        for k, v in self.netF.named_parameters():
            param_group_g += [{"params": v, "lr": lr * 1.0}]
        for k, v in self.netD.named_parameters():
            param_group_d += [{"params": v, "lr": lr * 2.0}]
        
        optimizer_g = optim.SGD(param_group_g)
        optimizer_d = optim.SGD(param_group_d)
        
        optimizer_g = op_copy(optimizer_g)
        optimizer_d = op_copy(optimizer_d)
        
        # åˆå§‹åŒ–ç¼“å­˜çš„æ–‡æœ¬ç‰¹å¾ï¼ˆç”¨äº KL è’¸é¦ï¼‰
        self.cached_text_features = None
        
        iter_num = 0
        iter_target = iter(dset_loaders["target"])
        max_iter = max_epoch * len(dset_loaders["target"])
        interval_iter = max_iter // interval
        
        # ä½¿ç”¨ tqdm è¿›åº¦æ¡
        pbar = tqdm(total=max_iter, desc=f"Target+CoOp [{task_name}]")
        
        # æ—©åœç›¸å…³å˜é‡
        best_acc = 0
        no_improve_count = 0
        best_iter = 0
        loss_mix = torch.tensor(0.0).to(self.device)
        
        while iter_num < max_iter:
            try:
                inputs_test, _, tar_idx = next(iter_target)
            except:
                iter_target = iter(dset_loaders["target"])
                inputs_test, _, tar_idx = next(iter_target)
            
            if inputs_test.size(0) == 1:
                continue
            
            # å®šæœŸæ›´æ–°ä¼ªæ ‡ç­¾ + ä¸¤é˜¶æ®µ Prompt Tuning
            if iter_num % interval_iter == 0 and cls_par > 0:
                self.netG.eval()
                self.netF.eval()
                
                # é˜¶æ®µ 1: æ›´æ–°ä¼ªæ ‡ç­¾
                mem_label1 = obtain_label(dset_loaders["test"], self.netG, self.netF, self.netC, device=self.device)
                mem_label2 = obtain_label_easy(dset_loaders["test"], self.netG, self.netD, device=self.device)
                mem_label1 = torch.from_numpy(mem_label1).to(self.device)
                mem_label2 = torch.from_numpy(mem_label2).to(self.device)
                
                # é˜¶æ®µ 2: ç‹¬ç«‹ Prompt Tuningï¼ˆæ”¶é›†é«˜ç½®ä¿¡åº¦æ ·æœ¬ + è®­ç»ƒ Promptï¼‰
                if self.coop_loss_weight > 0:
                    print(f"\n[Iter {iter_num}] å¼€å§‹ä¸¤é˜¶æ®µ Prompt Tuning...")
                    high_conf_indices, high_conf_probs = self._collect_high_confidence_samples(dset_loaders["target"])
                    if len(high_conf_indices) > 0:
                        self._tune_prompt_only(dset_loaders, high_conf_indices, high_conf_probs, log_file)
                    else:
                        print("  è­¦å‘Š: æ²¡æœ‰é«˜ç½®ä¿¡åº¦æ ·æœ¬ï¼Œè·³è¿‡ Prompt Tuning")
                        # å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œä½¿ç”¨åˆå§‹æ–‡æœ¬ç‰¹å¾
                        if self.cached_text_features is None:
                            with torch.no_grad():
                                self.cached_text_features = self._get_text_features()
                
                self.netG.train()
                self.netF.train()
            
            inputs_test = inputs_test.to(self.device)
            iter_num += 1
            pbar.update(1)
            
            lr_scheduler(optimizer_g, iter_num=iter_num, max_iter=max_iter)
            lr_scheduler(optimizer_d, iter_num=iter_num, max_iter=max_iter)
            
            # Step A: è®­ç»ƒ netD
            total_loss1 = 0
            features_d = self.netG(inputs_test)
            features = self.netF(features_d)
            outputs1 = self.netC(features)
            outputs2 = self.netD(features_d)
            
            softmax_out1 = nn.Softmax(dim=1)(outputs1)
            softmax_out2 = nn.Softmax(dim=1)(outputs2)
            
            loss_skl = torch.mean(torch.sum(SKL(softmax_out1, softmax_out2), dim=1))
            total_loss1 += loss_skl * 0.1
            
            loss_ent = entropy(self.netD, features_d, lamda)
            total_loss1 += loss_ent
            
            optimizer_d.zero_grad()
            total_loss1.backward()
            optimizer_d.step()
            
            # Step B: è”åˆè®­ç»ƒ netG + netD + PromptLearner (å¸¦ CoOp è’¸é¦)
            total_loss2 = 0
            features_d = self.netG(inputs_test)
            features = self.netF(features_d)
            outputs1 = self.netC(features)
            outputs2 = self.netD(features_d)
            
            softmax_out1 = nn.Softmax(dim=1)(outputs1)
            softmax_out2 = nn.Softmax(dim=1)(outputs2)
            
            # ä¼ªæ ‡ç­¾åˆ†ç±»æŸå¤±
            pred1 = mem_label1[tar_idx]
            pred2 = mem_label2[tar_idx]
            
            classifier_loss1 = nn.CrossEntropyLoss()(outputs1, pred1)
            classifier_loss2 = nn.CrossEntropyLoss()(outputs2, pred2)
            
            # ä¸ç¡®å®šæ€§åŠ æƒ
            kl_distance = nn.KLDivLoss(reduction='none')
            log_sm = nn.LogSoftmax(dim=1)
            softmax_out1_stable = torch.clamp(softmax_out1, min=1e-8)
            softmax_out2_stable = torch.clamp(softmax_out2, min=1e-8)
            variance1 = torch.sum(kl_distance(log_sm(outputs1), softmax_out2_stable), dim=1)
            variance2 = torch.sum(kl_distance(log_sm(outputs2), softmax_out1_stable), dim=1)
            
            exp_variance1 = torch.mean(torch.exp(-variance1))
            exp_variance2 = torch.mean(torch.exp(-variance2))
            
            # ä¸¤é˜¶æ®µ CoOp è’¸é¦ï¼šä½¿ç”¨ç¼“å­˜çš„æ–‡æœ¬ç‰¹å¾ï¼ˆPrompt åœ¨ç‹¬ç«‹é˜¶æ®µå·²è®­ç»ƒï¼‰
            loss_coop = torch.tensor(0.0).to(self.device)
            
            if self.cached_text_features is not None and self.coop_loss_weight > 0:
                # é€‰æ‹©éš¾æ ·æœ¬è¿›è¡Œ KL è’¸é¦
                entropy1 = -torch.sum(softmax_out1 * torch.log(softmax_out1 + 1e-8), dim=1)
                entropy2 = -torch.sum(softmax_out2 * torch.log(softmax_out2 + 1e-8), dim=1)
                avg_entropy = (entropy1 + entropy2) / 2
                
                max_entropy = np.log(self.class_num)
                entropy_threshold = max_entropy * self.coop_entropy_ratio
                
                discrepancy = torch.sum(SKL(softmax_out1, softmax_out2), dim=1)
                discrepancy_threshold = discrepancy.quantile(1 - self.coop_top_k_ratio)
                
                high_entropy_mask = avg_entropy > entropy_threshold
                high_discrepancy_mask = discrepancy > discrepancy_threshold
                difficult_mask = high_entropy_mask | high_discrepancy_mask
                topk_indices = torch.where(difficult_mask)[0]
                
                if len(topk_indices) > 0:
                    clip_input = F.interpolate(inputs_test[topk_indices], size=(224, 224), mode='bicubic')
                    
                    # è·å– CLIP å›¾åƒç‰¹å¾ï¼ˆå†»ç»“ï¼‰
                    with torch.no_grad():
                        image_features = self.clip_model.encode_image(clip_input)
                        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
                        
                        # ä½¿ç”¨ç¼“å­˜çš„æ–‡æœ¬ç‰¹å¾ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼ŒPrompt ä¸æ›´æ–°ï¼‰
                        clip_logits = image_features @ self.cached_text_features.T
                        clip_probs = F.softmax(clip_logits.float() * self.coop_temperature, dim=1)
                    
                    # KL è’¸é¦æŸå¤±ï¼ˆåªè®­ç»ƒ DCA ç½‘ç»œï¼Œæ¢¯åº¦ä¸æµå‘ Promptï¼‰
                    clip_confidence = clip_probs.max(dim=1)[0]
                    weight = clip_confidence / clip_confidence.mean()
                    
                    kl_per_sample1 = F.kl_div(log_sm(outputs1[topk_indices]), clip_probs, reduction='none').sum(dim=1)
                    kl_per_sample2 = F.kl_div(log_sm(outputs2[topk_indices]), clip_probs, reduction='none').sum(dim=1)
                    
                    loss_coop = (weight * (kl_per_sample1 + kl_per_sample2)).mean()
            
            total_loss2 += self.coop_loss_weight * loss_coop
            
            # åŠ æƒåˆ†ç±»æŸå¤±
            loss_seg1 = classifier_loss1 * exp_variance1 + torch.mean(variance1)
            loss_seg2 = classifier_loss2 * exp_variance2 + torch.mean(variance2)
            classifier_loss = alpha * loss_seg1 + (2 - alpha) * loss_seg2
            loss_cs = cls_par * classifier_loss
            total_loss2 += loss_cs
            
            # å¯¹æŠ—ç†µæŸå¤±
            loss_ent1 = adentropy(self.netC, features, lamda)
            loss_ent2 = adentropy(self.netD, features_d, lamda)
            loss_mme = loss_ent1 + loss_ent2
            total_loss2 += loss_mme
            
            # ç±»å¹³è¡¡æŸå¤±
            loss_cb1 = class_balance(softmax_out1, lamda)
            loss_cb2 = class_balance(softmax_out2, lamda)
            loss_cb = loss_cb1 + loss_cb2
            total_loss2 += loss_cb
            
            # MixUp æ•°æ®å¢å¼º
            if mix > 0:
                alpha_mix = 0.3
                lam = np.random.beta(alpha_mix, alpha_mix)
                index = torch.randperm(inputs_test.size()[0]).to(self.device)
                mixed_input = lam * inputs_test + (1 - lam) * inputs_test[index, :]
                mixed_softout = (lam * softmax_out1 + (1 - lam) * softmax_out2[index, :]).detach()
                
                features_mix = self.netG(mixed_input)
                outputs_mixed1 = self.netC(self.netF(features_mix))
                outputs_mixed2 = self.netD(features_mix)
                
                outputs_mixed_softmax1 = torch.nn.Softmax(dim=1)(outputs_mixed1)
                outputs_mixed_softmax2 = torch.nn.Softmax(dim=1)(outputs_mixed2)
                
                loss_mix1 = mix * nn.KLDivLoss(reduction='batchmean')(outputs_mixed_softmax1.log(), mixed_softout)
                loss_mix2 = mix * nn.KLDivLoss(reduction='batchmean')(outputs_mixed_softmax2.log(), mixed_softout)
                loss_mix = loss_mix1 + loss_mix2
                total_loss2 += loss_mix
            
            # æ›´æ–° DCA ç½‘ç»œå‚æ•°ï¼ˆä¸¤é˜¶æ®µç­–ç•¥ï¼šPrompt åœ¨ç‹¬ç«‹é˜¶æ®µå·²æ›´æ–°ï¼Œè¿™é‡Œä¸æ›´æ–°ï¼‰
            optimizer_g.zero_grad()
            optimizer_d.zero_grad()
            total_loss2.backward()
            optimizer_g.step()
            optimizer_d.step()
            
            # å®šæœŸè¯„ä¼°
            if iter_num % interval_iter == 0 or iter_num == max_iter:
                self.netG.eval()
                self.netF.eval()
                
                _, acc_list1, accuracy1 = cal_acc(dset_loaders["test"], self.netG, self.netF, self.netC, device=self.device)
                _, acc_list2, accuracy2 = cal_acc_easy(dset_loaders["test"], self.netG, self.netD, device=self.device)
                
                log_str = f"Iter:{iter_num}/{max_iter}; Acc_c={accuracy1:.2f}%, Acc_d={accuracy2:.2f}%"
                if self.verbose_loss:
                    log_str += f"; Lcls:{loss_cs.item():.4f}; Lcoop:{loss_coop.item():.4f}; Lmme:{loss_mme.item():.4f}; Lcb:{loss_cb.item():.4f}; Lmix:{loss_mix.item():.4f}"
                
                log_file.write(log_str + "\n")
                log_file.flush()
                
                pbar.set_postfix({'Acc_c': f'{accuracy1:.2f}%', 'Acc_d': f'{accuracy2:.2f}%'})
                
                # æ£€æŸ¥æ˜¯å¦æå‡å¹¶ä¿å­˜æœ€ä½³æ¨¡å‹
                if accuracy1 > best_acc:
                    best_acc = accuracy1
                    best_iter = iter_num
                    no_improve_count = 0
                    if self.config.get('savemodel', True):
                        torch.save(self.netG.state_dict(), osp.join(output_dir, "target_G.pt"))
                        torch.save(self.netF.state_dict(), osp.join(output_dir, "target_F.pt"))
                        torch.save(self.netC.state_dict(), osp.join(output_dir, "target_C.pt"))
                        torch.save(self.netD.state_dict(), osp.join(output_dir, "target_D.pt"))
                        torch.save(self.prompt_learner.state_dict(), osp.join(output_dir, "prompt_learner.pt"))
                        log_file.write(f"  -> æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Acc={accuracy1:.2f}%)\n")
                        log_file.flush()
                else:
                    no_improve_count += 1
                    log_file.write(f"  -> æœªæå‡ ({no_improve_count}/{early_stop_patience})\n")
                    log_file.flush()
                
                # æ—©åœæ£€æŸ¥
                if early_stop_enabled and no_improve_count >= early_stop_patience:
                    log_file.write(f"\næ—©åœè§¦å‘: è¿ç»­ {early_stop_patience} æ¬¡è¯„ä¼°æœªæå‡\n")
                    log_file.write(f"æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}% @ iter {best_iter}\n")
                    log_file.flush()
                    print(f"\næ—©åœè§¦å‘! æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}% @ iter {best_iter}")
                    break
                
                self.netG.train()
                self.netF.train()
        
        pbar.close()
        
        # å†™å…¥ç»“æŸæ—¶é—´
        self._write_end_time(log_file, start_time)
        
        # TTA æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰
        if self.config.get('use_tta', True):
            print("\nè¿›è¡Œ TTA æœ€ç»ˆè¯„ä¼°...")
            log_file.write("\n" + "=" * 60 + "\n")
            log_file.write("ğŸ¯ TTA æœ€ç»ˆè¯„ä¼°\n")
            log_file.write("=" * 60 + "\n")
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            if self.config.get('savemodel', True):
                self.netG.load_state_dict(torch.load(osp.join(output_dir, "target_G.pt"), map_location=self.device))
                self.netF.load_state_dict(torch.load(osp.join(output_dir, "target_F.pt"), map_location=self.device))
                self.netC.load_state_dict(torch.load(osp.join(output_dir, "target_C.pt"), map_location=self.device))
                self.netD.load_state_dict(torch.load(osp.join(output_dir, "target_D.pt"), map_location=self.device))
            
            self.netG.eval()
            self.netF.eval()
            
            # æ™®é€šè¯„ä¼°
            _, _, acc_normal = cal_acc(dset_loaders["test"], self.netG, self.netF, self.netC, device=self.device)
            
            # TTA è¯„ä¼°
            _, _, acc_tta = cal_acc_tta(dset_loaders["test"], self.netG, self.netF, self.netC, device=self.device)
            
            improvement = acc_tta - acc_normal
            log_file.write(f"æ™®é€šè¯„ä¼°: {acc_normal:.2f}%\n")
            log_file.write(f"TTA è¯„ä¼°: {acc_tta:.2f}%\n")
            log_file.write(f"TTA æå‡: {improvement:+.2f}%\n")
            log_file.write("=" * 60 + "\n")
            log_file.flush()
            
            print(f"TTA è¯„ä¼°å®Œæˆ: {acc_normal:.2f}% -> {acc_tta:.2f}% ({improvement:+.2f}%)")
            best_acc = max(best_acc, acc_tta)
        
        print(f"ç›®æ ‡åŸŸé€‚åº”å®Œæˆ (å¸¦ CoOp): {task_name}! æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
        return output_dir
